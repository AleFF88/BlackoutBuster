Потрыібно організувати повідомлення про графіки відключення.
Інформація є на сайті ЗапоріжОблЕнерго, її треба звідти діставати та відправляти до Telegram. Також я не хочу возитися з домашнім сервером, тому я планую організувати періодичний запуск відразу на Гіті через Actions.
Я вирішив оформити це завдання з короткими поясненнями для мого друга, щоб він міг використовувати його як навчальний посібник.

Нам знадобиться консольний додаток, який періодично перевірятиме сторінку ПАТ «Запоріжжяобленерго» (https://www.zoe.com.ua/outage/) на предмет оновлень. Оскільки сайт використовує специфічну структуру, ми орієнтуватимемося на пошук конкретних ключових слів.

Будемо використовувати HtmlAgilityPack для парсингу сторінки (додамо NuGet).
Специфіка сайту: Сайт "Запоріжобленерго" віддає статичний HTML. Щоб програма зрозуміла, де знаходиться текст групи «5.1», їй потрібно спочатку отримати весь текст сторінки та розпарити його дерево (DOM). Жаль, що немає API.
Ще можна було б перевіряти заголовок If-Modified-Since, щоб не качати сторінку, якщо вона не змінилася. Але сайт обленерго, схоже, його не використовує.

У коді дано докладні коментарі, але дещо доповню.
targetHeaderNode
1) Пошук йде до першого збігу з умовами. FirstOrDefault зупиняє пошук, коли знаходить перший вузол, який відповідає. Це зроблено тому, що на сайті останні за часом дані вгорі.
2) Навіщо потрібний Descendants()?
Метод Descendants() («Нащадки») проходить по всьому дереву HTML-документа зверху донизу, заглядаючи у кожен вкладений тег (рекурсивно). На сайті обленерго потрібний заголовок може бути глибоко захований: body -> div -> main -> article -> p. Descendants() знайде його, де б він не знаходився. Він заходить у перший <div>, перевіряє всі його вкладені елементи, потім переходить до другого тощо.
Альтернатива: Метод ChildNodes бере лише «нащадків» першого рівня. Нам тут не підійде.
3) Іноді на сторінці трапляються величезні блоки (наприклад, скрипти <script> або приховані <div>), які всередині свого тексту можуть містити поєднання слів «ГПВ» та «оновлено» (наприклад, в описі правил або SEO-тексту). Логіка: Заголовок новини або графіка зазвичай короткий (один рядок). Якщо довжина тексту у вузлі більше 200 символів, то швидше за все це не заголовок.

parentContainer
Справа у специфіці верстки сайту «Запоріжжяобленерго». Зазвичай заголовок (наприклад, тег <h3> або <strong> з текстом "ГПВ оновлено") і сам список черг (1.1, 2.2 і т.д.) - це сусідні елементи або елементи, що знаходяться в одному спільному "контейнері" (диві).
Якщо ми знайшли вузол із текстом "ГПВ", то інформація про черги (5.1) знаходиться не всередині цього тексту, а поруч із ним або трохи нижче у структурі документа. Щоб «побачити» і заголовок, і дані груп, нам потрібно піднятися на рівень вище — до їхнього спільного «предка» (контейнера), який об'єднує весь блок новин про відключення.

Далі про пошук "вгору по дереву". Як це працює в коді:
	Ми стоїмо на вузлі "Заголовок".
	Запитуємо: «Тут є текст '5.1'?» - Відповідь: "Ні".
	Робимо крок вгору до предка (наприклад, блок <div>).
	Запитуємо: «А всередині цього предка (у всіх його дочірніх елементах сумарно) є текст '5.1'?»
	Якщо "Так" - стоп, ми знайшли спільний контейнер, з якого тепер можна взяти потрібний шматок тексту.
Якщо цикл дійшов до тегів <body>, <html> або класу main-wrapper і так і не знайшов тексту «5.1», це означає одне: заголовок про оновлення графіків є, а самих даних за твоєю групою на сторінці немає (або вони написані в якомусь зовсім іншому форматі, який ми не передбачили).

На цьому етапі консольний додаток успішно працює та дістає актуальну інформацію щодо графіку відключень для конкретної групи.




Додав GitHub Actions для запуску за розкладом.
При ручному запуску на гіті отримав
Error: : The request був canceled due to the configured HttpClient.Timeout of 100 seconds elapsing

Сайти багатьох українських держструктур та обленерго зараз знаходяться під захистом від DDoS-атак і часто блокують запити, що виходять з-за кордону (а сервери GitHub Actions знаходяться у США чи Європі). Коли сайт бачить запит з IP-адреси дата-центру Microsoft (GitHub), він просто "мовчить", поки у твого HttpClient не буде тайм-аут через 100 секунд.

Для вирішення я пропоную використовувати ScraperAPI (https://status.scraperapi.com/) - це спеціалізований HTTP-проксі-шлюз, який є посередником між додатком і цільовим сервером. В даному випадку він вирішує проблему блокування GeoIP і фільтрації трафіку з дата-центрів. 
	Resident Proxy Network: Сервіс пересилає твій запит не через серверну стійку, а через пул резидентських IP-адрес (реальних інтернет-провайдерів України). Для системи захисту zoe.com.ua такий запит має вигляд легітимного трафіку від домашнього користувача. 
	Request Decoration: ScraperAPI автоматично підміняє та ротує заголовки (Headers), такі як Accept-Language, Referer та Cipher Suites (в рамках TLS-рукостискання), щоб вони відповідали сучасним браузерам. 
	Bypass Anti-Bot: При параметрі premium=true сервіс використовує механізми імітації виконання JS-коду або обходу перевірки заголовків на рівні TCP/IP (Fingerprinting), що запобігає детектуванню автоматизованого ПЗ.

Технічно ми перейдемо від моделі прямого з'єднання до моделі роботи з проксі через API. Ми замість нашого url передаємо в httpClient рядок, який містить все необхідне для скраппера. Запит спочатку піде на сайт скраппера. Скраппер сам отримає дані від сайту ЗОЕ та передасть їх нам як відповідь від httpClient.

Технічний цикл запиту 
	Request Construction: Твій код упаковує цільову URL (TargetUrl) як параметр рядка запиту для ScraperAPI. 
	Handshake: HttpClient визначає з'єднання з api.scraperapi.com. 
	Instruction Delivery: Ти передаєш ScraperAPI інструкції: «Мені потрібна ця URL, використовуй резидентний IP України та преміум-функції». 
	Remote Execution: Сервер ScraperAPI (перебуваючи у своїй інфраструктурі) ініціює новий HTTP-запит до zoe.com.ua. 
	Data Relay: ScraperAPI отримує HTML-відповідь від Обленерго, може очистити його від деяких скриптів або капч і повертає цей контент у тілі HTTP-відповіді на вихідний запит.

Я оновив метод, додавши обробку змінної оточення та коректне складання проксі-рядка.
І оновив .yml файл, тому що потрібно передати цей секрет у програму.

Важливий момент! Для безпеки ми не пишемо ключ у коді, а беремо його зі змінної оточення.
Як додати ключ до GitHub: 
	Зайди у свій репозиторій -> Settings -> Secrets and variables -> Actions. 
	Натисні New repository secret. 
	Назви його SCRAPER_API_KEY, а в полі Value встав свій ключ від ScraperAPI.
На ПК аналогічно через Edit environment variables (перезапусти Visual Studio).




Тепер налаштуємо пересилання в телеграм.
Знайди в пошуку Telegram користувача @BotFather (це офіційний бот для створення інших роботів).
/start > /newbot
Задай назву та ім'я. Отримай токен для доступу (збережи)


Налаштування каналу
Створи публічний або приватний канал у Telegram.
Перевір налаштування каналу -> Адміністратори. 
Знайди свого бота за юзернеймом і додай його. Йому достатньо права на "Публікацію повідомлень".
Дізнатися ID каналу: 
	Якщо канал публічний, його ID — його адреса (наприклад, @my_channel_name). 
	Якщо приватний, найпростіше переслати будь-яке повідомлення з цього каналу бот @userinfobot, і він скаже тобі ID (зазвичай починається з -100 ...).	
В коде аналогично токену для скраппера задай токен для Телеграма.


При надсиланні повідомлення в телеграм я знов використовую HttpClient. Зверні увагу, що тепер я його зробив одним на всю програму і перевикористовую в потрібних місцях.
Технічне обґрунтування 
	TIME_WAIT: Операційна система тримає сокет «напіввідкритим» ще деякий час (зазвичай від 30 до 240 секунд), щоб переконатися, що всі пакети даних, що затрималися, з мережі дійшли до адресата і не переплуталися з новим з'єднанням. 
	Socket Exhaustion (Вичерпання сокетів): Якби програма робила тисячі запитів і щоразу створювала новий HttpClient, вона б швидко забила всі доступні порти цими сокетами, що «ждуть», і видала б помилку. 
	Keep-Alive: Коли використовуємо той самий HttpClient, він намагається перевикористовувати вже відкрите з'єднання (Keep-Alive). Це набагато швидше, тому що не потрібно витрачати час на "рукостискання" TCP та встановлення TLS-шифрування для кожного нового запиту.
З урахуванням, що програма працює пару хвилин немає сенсу відкривати два з'єднання для HttpClient.
	
	
	
	
Протестував, все працює. Хочеться тепер зробити так, щоб не було повторних повідомлень. Для цього потрібно зберегти крайній апдейт і порівнювати - чи з'явилося те, що треба повідомити або дані не змінилися.
Я імплементував таке порівняння, додав збереження даних у файл, змінив налаштування гіта, щоб він дозволяв GitHub Actions записувати у файл.
Коли надсилаються повідомлення
1) файлу з попереднім станом немає (перший запуск, наприклад)
2) змінилися день щодо минулого стан (навіть якщо графік той самий треба повідомити, що сьогодні він ось такий)
3) Змінився заголовок (відновлення) та змінився графік. Якщо оновлення буде змінено, але для 5.1 нічого не змінюється, не буде зайвого повідомлення з тим же графіком.

Зверні увагу, що в IsUpdateRequired я використав if (loadedData is not ScheduleState saved). Чому не (loadedData == null)?
Коли оголошуємо ScheduleState?, C# перетворює це на спеціальну системну структуру:
public struct Nullable<T> where T : struct { 
	private T value; // Сама структура (прихована всередині) 
	private bool hasValue; // Прапор: є всередині дані або там "порожньо" 
	public T Value => hasValue ? value : throw new InvalidOperationException(); 
	public bool HasValue => hasValue;
}

Після цього блоку loadedState — це звичайна структура типу ScheduleState (не nullable). Ми можемо звертатися до полів безпосередньо без .Value. Отримуємо безпеку (перевірка на null) та зручність (створення локальної змінної без Nullable).

Чи є сенс у record struct чи просто struct? 
record struct тут краще, ніж звичайна struct: 
	Порівняння об'єктів (Value-based equality) "з коробки": 
		У методі IsUpdateRequired ми могли б просто написати if (loadedData == currentState). Для record struct C# сам згенерує код, який порівняє кожне поле. Для звичайної структури довелося б перевизначати оператор == вручну або використовувати повільну рефлексію. 
	Незмінність (readonly): 
		Використовуєш readonly record struct. Це ідеальний патерн для DTO (Data Transfer Object). Один раз отримав дані із сайту, і вони не повинні змінюватися в процесі роботи. 
		До того ж, потенційно не створюються захисні копії при передачі в методи. 
			Але in я не став писати, оскільки структура крихітна. Копіювання тут не є вузьким місцем. Копіювання в стеку швидше за все відбувається швидше, ніж створення та розйменування посилання.

Щоб гіт міг зберігати файли, треба дати йому команду комітити змінений файл. Відображено в run-checker.yml
	Також треба налаштувати дозвіл на самому гіті, Workflow Permissions: Settings -> Actions -> General. 
		Перейди до Workflow permissions і переконайся, що вибрано "Read and write permissions". 
		Увімкни чекбокс Allow GitHub Actions для створення та approve pull requests.