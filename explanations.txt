Потрыібно організувати повідомлення про графіки відключення.
Інформація є на сайті ЗапоріжОблЕнерго, її треба звідти діставати та відправляти до Telegram. Також я не хочу возитися з домашнім сервером, тому я планую організувати періодичний запуск відразу на Гіті через Actions.
Я вирішив оформити це завдання з короткими поясненнями для мого друга, щоб він міг використовувати його як навчальний посібник.

Нам знадобиться консольний додаток, який періодично перевірятиме сторінку ПАТ «Запоріжжяобленерго» (https://www.zoe.com.ua/outage/) на предмет оновлень. Оскільки сайт використовує специфічну структуру, ми орієнтуватимемося на пошук конкретних ключових слів.

Будемо використовувати HtmlAgilityPack для парсингу сторінки (додамо NuGet).
Специфіка сайту: Сайт "Запоріжобленерго" віддає статичний HTML. Щоб програма зрозуміла, де знаходиться текст групи «5.1», їй потрібно спочатку отримати весь текст сторінки та розпарити його дерево (DOM). Жаль, що немає API.
Ще можна було б перевіряти заголовок If-Modified-Since, щоб не качати сторінку, якщо вона не змінилася. Але сайт обленерго, схоже, його не використовує.

У коді дано докладні коментарі, але дещо доповню.
targetHeaderNode
1) Пошук йде до першого збігу з умовами. FirstOrDefault зупиняє пошук, коли знаходить перший вузол, який відповідає. Це зроблено тому, що на сайті останні за часом дані вгорі.
2) Навіщо потрібний Descendants()?
Метод Descendants() («Нащадки») проходить по всьому дереву HTML-документа зверху донизу, заглядаючи у кожен вкладений тег (рекурсивно). На сайті обленерго потрібний заголовок може бути глибоко захований: body -> div -> main -> article -> p. Descendants() знайде його, де б він не знаходився. Він заходить у перший <div>, перевіряє всі його вкладені елементи, потім переходить до другого тощо.
Альтернатива: Метод ChildNodes бере лише «нащадків» першого рівня. Нам тут не підійде.
3) Іноді на сторінці трапляються величезні блоки (наприклад, скрипти <script> або приховані <div>), які всередині свого тексту можуть містити поєднання слів «ГПВ» та «оновлено» (наприклад, в описі правил або SEO-тексту). Логіка: Заголовок новини або графіка зазвичай короткий (один рядок). Якщо довжина тексту у вузлі більше 200 символів, то швидше за все це не заголовок.

parentContainer
Справа у специфіці верстки сайту «Запоріжжяобленерго». Зазвичай заголовок (наприклад, тег <h3> або <strong> з текстом "ГПВ оновлено") і сам список черг (1.1, 2.2 і т.д.) - це сусідні елементи або елементи, що знаходяться в одному спільному "контейнері" (диві).
Якщо ми знайшли вузол із текстом "ГПВ", то інформація про черги (5.1) знаходиться не всередині цього тексту, а поруч із ним або трохи нижче у структурі документа. Щоб «побачити» і заголовок, і дані груп, нам потрібно піднятися на рівень вище — до їхнього спільного «предка» (контейнера), який об'єднує весь блок новин про відключення.

Далі про пошук "вгору по дереву". Як це працює в коді:
	Ми стоїмо на вузлі "Заголовок".
	Запитуємо: «Тут є текст '5.1'?» - Відповідь: "Ні".
	Робимо крок вгору до предка (наприклад, блок <div>).
	Запитуємо: «А всередині цього предка (у всіх його дочірніх елементах сумарно) є текст '5.1'?»
	Якщо "Так" - стоп, ми знайшли спільний контейнер, з якого тепер можна взяти потрібний шматок тексту.
Якщо цикл дійшов до тегів <body>, <html> або класу main-wrapper і так і не знайшов тексту «5.1», це означає одне: заголовок про оновлення графіків є, а самих даних за твоєю групою на сторінці немає (або вони написані в якомусь зовсім іншому форматі, який ми не передбачили).

На цьому етапі консольний додаток успішно працює та дістає актуальну інформацію щодо графіку відключень для конкретної групи.




Додав GitHub Actions для запуску за розкладом.
При ручному запуску на гіті отримав
Error: : The request був canceled due to the configured HttpClient.Timeout of 100 seconds elapsing

Сайти багатьох українських держструктур та обленерго зараз знаходяться під захистом від DDoS-атак і часто блокують запити, що виходять з-за кордону (а сервери GitHub Actions знаходяться у США чи Європі). Коли сайт бачить запит з IP-адреси дата-центру Microsoft (GitHub), він просто "мовчить", поки у твого HttpClient не буде тайм-аут через 100 секунд.

Для вирішення я пропоную використовувати ScraperAPI - це спеціалізований HTTP-проксі-шлюз, який є посередником між додатком і цільовим сервером. В даному випадку він вирішує проблему блокування GeoIP і фільтрації трафіку з дата-центрів. 
	Resident Proxy Network: Сервіс пересилає твій запит не через серверну стійку, а через пул резидентських IP-адрес (реальних інтернет-провайдерів України). Для системи захисту zoe.com.ua такий запит має вигляд легітимного трафіку від домашнього користувача. 
	Request Decoration: ScraperAPI автоматично підміняє та ротує заголовки (Headers), такі як Accept-Language, Referer та Cipher Suites (в рамках TLS-рукостискання), щоб вони відповідали сучасним браузерам. 
	Bypass Anti-Bot: При параметрі premium=true сервіс використовує механізми імітації виконання JS-коду або обходу перевірки заголовків на рівні TCP/IP (Fingerprinting), що запобігає детектуванню автоматизованого ПЗ.

Технічно ми перейдемо від моделі прямого з'єднання до моделі роботи з проксі через API. Ми замість нашого url передаємо в httpClient рядок, який містить все необхідне для скраппера. Запит спочатку піде на сайт скраппера. Скраппер сам отримає дані від сайту ЗОЕ та передасть їх нам як відповідь від httpClient.

Технічний цикл запиту 
	Request Construction: Твій код упаковує цільову URL (TargetUrl) як параметр рядка запиту для ScraperAPI. 
	Handshake: HttpClient визначає з'єднання з api.scraperapi.com. 
	Instruction Delivery: Ти передаєш ScraperAPI інструкції: «Мені потрібна ця URL, використовуй резидентний IP України та преміум-функції». 
	Remote Execution: Сервер ScraperAPI (перебуваючи у своїй інфраструктурі) ініціює новий HTTP-запит до zoe.com.ua. 
	Data Relay: ScraperAPI отримує HTML-відповідь від Обленерго, може очистити його від деяких скриптів або капч і повертає цей контент у тілі HTTP-відповіді на вихідний запит.

Я оновив метод, додавши обробку змінної оточення та коректне складання проксі-рядка.
І оновив .yml файл, тому що потрібно передати цей секрет у програму.

Важливий момент! Для безпеки ми не пишемо ключ у коді, а беремо його зі змінної оточення.
Як додати ключ до GitHub: 
	Зайди у свій репозиторій -> Settings -> Secrets and variables -> Actions. 
	Натисні New repository secret. 
	Назви його SCRAPER_API_KEY, а в полі Value встав свій ключ від ScraperAPI.
На ПК аналогічно через Edit environment variables (перезапусти Visual Studio)